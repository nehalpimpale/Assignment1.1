# Assignment1.1

1. Various sources of Big Data : 

    1.Archieves
     - Internal archived data that lives behind your own firewalls is typically unstructured, and uses no APIs
    2.Documents
     - Docs can exist inside or outside your organization
    3.Media
      -exists in-and-out of your organization, may connect with APIs
      -is moderately structured.
    4.Data Storage
    5.Business apps
      -Business apps are structured
    6.Public websites
      -Public web is external, but some very cool and useful applications can be mashed up with it. For example, is your business affected        by the daily fluctuation of currency
    7.Social media
      -Social media is high velocity, high volume data that you can use to detect trends, analyze sentiment about your brand, customer            service and competitors, or target campaigns to social accounts that match the email addresses in your customer file 
    8.Machine log data
      -You’re likely making good use of machine log data through your Web analytics, the next step is using mobile or third party services        that help you better identify, target and convert visitors.
    9.Sensor data
      -Sensor data is high velocity, volume, variety and dare I add…value, when used correctly to understand user context and predict            behavior. Sensors for geolocation, temperature, noise, attention, engagement, biometrics
      
      
2. 3 V's of Big Data :
      
      3Vs (volume, variety and velocity) are three defining properties or dimensions of big data. Volume refers to the amount of data, variety refers to the number of types of data and velocity refers to the speed of data processing. According to the 3Vs model, the challenges of big data management result from the expansion of all three properties, rather than just the volume alone -- the sheer amount of data to be managed.
     
Data Volume:
The size of available data has been growing at an increasing rate. This applies to companies and to individuals. A text file is a few kilo bytes, a sound file is a few mega bytes while a full length movie is a few giga bytes.
More sources of data are added on continuous basis. For companies, in the old days, all data was generated internally by employees. Currently, the data is generated by employees, partners and customers. For a group of companies, the data is also generated by machines

Data Velocity:
Initially, companies analyzed data using a batch process. One takes a chunk of data, submits a job to the server and waits for delivery of the result. That scheme works when the incoming data rate is slower than the batch processing rate and when the result is useful despite the delay. With the new sources of data such as social and mobile applications, the batch process breaks down. The data is now streaming into the server in real time, in a continuous fashion and the result is only useful if the delay is very short.

Data Variety:
From excel tables and databases, data structure has changed to loose its structure and to add hundreds of formats. Pure text, photo, audio, video, web, GPS data, sensor data, relational data bases, documents, SMS, pdf, flash, etc   


3. Horizontal Scaling and Vertical Scaling

(1).Scale-Out or Horizontal Scaling

When you add more servers with less RAM and processors, it is known as horizontal scaling. It can also be defined as the ability to increase the capacity by connecting multiple software or hardware entities in such a manner that they function as a single logical unit. It is cheaper as a whole and it can literally scale infinitely, however, there are some limits imposed by software or other attributes of an environment’s infrastructure. When the servers are clustered, the original server is scaled out horizontally. If a cluster requires more resources to improve its performance and provide high availability, then the administrator can scale-out by adding more servers to the cluster.

Pros of Scaling-out

Much cheaper compared to scaling-up
Takes advantage of smaller systems
Easy to upgrade
Resilience is improved due to the presence of discrete, multiple systems
Easier to run fault-tolerance
Supporting linear increases in capacity

Cons of Scaling-out

The licensing fees are more
Utility costs such as cooling and electricity are high
It has a bigger footprint in the Data Center
More networking equipment such as routers and switches may be needed


(2). Scale-up or Vertical Scaling
It refers to the process of adding more physical resources such as memory, storage and CPU to the existing database server for improving the performance. Vertical scaling helps in upgrading the capacity of the existing database server. It results in a robust system. Some of its pros include:

Pros of Scaling-Up

It consumes less power as compared to running multiple servers
Administrative efforts will be reduced as you need to handle and manage just one system
Cooling costs are lesser than horizontal scaling
Reduced software costs
Implementation isn’t difficult
The licensing costs are less
The application compatibility is retained

Cons of Scaling up

There is a greater risk of hardware failure which can cause bigger outages
Limited scope of upgradeability in the future
Severe vendor lock-in
The overall cost of implementing is really expensive


4. Need and Working of Hadoop


The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly available service on top of a cluster of computers, each of which may be prone to failures.
It has two main parts – a data processing framework and a distributed filesystem for data storage.

(1).Hadoop Distributed Filesystem (HDFS) : 

The distributed filesystem is that far-flung array of storage clusters noted above – i.e., the Hadoop component that holds the actual data. By default, Hadoop uses the cleverly named Hadoop Distributed File System (HDFS)

(2).Data Processing Framework & MapReduce :

The data processing framework is the tool used to work with the data itself. By default, this is the Java-based system known as MapReduce. You hear more about MapReduce than the HDFS side of Hadoop for two reasons:

It’s the tool that actually gets data processed.
It tends1 to drive people slightly crazy when they work with it.
In a “normal” relational database, data is found and analyzed using queries, based on the industry-standard Structured Query Language (SQL). Non-relational databases use queries, too; they’re just not constrained to use only SQL, but can use other query languages to pull information out of data stores. Hence, the term NoSQL.

(3).Scattered Across The Cluster :
There is another element of Hadoop that makes it unique: All of the functions described act as distributed systems, not the more typical centralized systems seen in traditional databases.On a Hadoop cluster, the data within HDFS and the MapReduce system are housed on every machine in the cluster. This has two benefits: it adds redundancy to the system in case one machine in the cluster goes down, and it brings the data processing software into the same machines where data is stored, which speeds information retrieval i.e.,Robust and efficient.


